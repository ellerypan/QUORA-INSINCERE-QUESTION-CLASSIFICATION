{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "abb7e3c30b8a412a50c6b451c49939e3cf4bc11b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "import torch\n",
    "from torchtext import data\n",
    "import spacy\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas(desc='Progress')\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from torchtext.data import Example\n",
    "from sklearn.metrics import f1_score\n",
    "import torchtext\n",
    "import os \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# cross validation and metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from unidecode import unidecode\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a4ff5590a6f152dc1bec5aeca79aef10218f7de"
   },
   "source": [
    "### Basic Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "deee49df5ca1c4413f71677939e26aa1ff784e44",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 120000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 70 # max number of words in a question to use\n",
    "batch_size = 512 # how many samples to process at once\n",
    "n_epochs = 5 # how many times to iterate over all samples\n",
    "# n_splits = 5 # Number of K-fold Splits\n",
    "\n",
    "SEED = 1029"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "654cbe3c8a1f2a618a2441afe00df3b4a89e0a58"
   },
   "source": [
    "### Ensure determinism in the results\n",
    "\n",
    "The is for the model built in Pytorch. Determinism is one of the advantages of Pytorch in this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "58bbf87335799247586aaed16531f4d28d10ed4a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c890692644acce2dc4f6e2f929d6d294faca4ad2"
   },
   "source": [
    "### Code for Loading Embeddings\n",
    "\n",
    "Functions taken from the kernel:https://www.kaggle.com/gmhost/gru-capsule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "7026ee1d913f54dd4b560f654efdb9f833581cd3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.005838499,0.48782197\n",
    "    embed_size = all_embs.shape[1]\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    # Why random embedding for OOV? what if use mean?\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    #embedding_matrix = np.random.normal(emb_mean, 0, (nb_words, embed_size)) # std 0\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix \n",
    "    \n",
    "def load_fasttext(word_index):    \n",
    "    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    #embedding_matrix = np.random.normal(emb_mean, 0, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.0053247833,0.49346462\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    #embedding_matrix = np.random.normal(emb_mean, 0, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea10c8e218a1280faa9802bcb7f1117c89ec96f9"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "173753f0178464d2ba26baf22899884d76d1c83d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/test.csv\")\n",
    "df = pd.concat([df_train ,df_test],sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "0f75559b6fa28c27ecbf145121309378afc884ee",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "vocab = build_vocab(df['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "5cb425ffbf1f79c1edc4cad3da15a1c1aa53edca",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Sincere questions: 1,225,312(93.81%) and # Insincere questions: 80,810(6.19%)\n",
      "# Test samples: 375,806(0.288% of train samples)\n"
     ]
    }
   ],
   "source": [
    "sin = len(df_train[df_train[\"target\"]==0])\n",
    "insin = len(df_train[df_train[\"target\"]==1])\n",
    "persin = (sin/(sin+insin))*100\n",
    "perinsin = (insin/(sin+insin))*100            \n",
    "print(\"# Sincere questions: {:,}({:.2f}%) and # Insincere questions: {:,}({:.2f}%)\".format(sin,persin,insin,perinsin))\n",
    "# print(\"Sinsere:{}% Insincere: {}%\".format(round(persin,2),round(perinsin,2)))\n",
    "print(\"# Test samples: {:,}({:.3f}% of train samples)\".format(len(df_test),len(df_test)/len(df_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07e9890ec0b490cef57565f7dff953aa56ebd3dc"
   },
   "source": [
    "# Preprocess\n",
    "\n",
    "Part of codes borrowed from\n",
    "* Improve your Score with some Text Preprocessing https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "28ebb28ba78972bb8d4fee9b53437045542d20fb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def known_contractions(embed):\n",
    "    known = []\n",
    "    for contract in contraction_mapping:\n",
    "        if contract in embed:\n",
    "            known.append(contract)\n",
    "    return known\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "\n",
    "def correct_spelling(x, dic):\n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "def unknown_punct(embed, punct):\n",
    "    unknown = ''\n",
    "    for p in punct:\n",
    "        if p not in embed:\n",
    "            unknown += p\n",
    "            unknown += ' '\n",
    "    return unknown\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "def clean_special_chars(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def add_lower(embedding, vocab):\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        if word in embedding and word.lower() not in embedding:  \n",
    "            embedding[word.lower()] = embedding[word]\n",
    "            count += 1\n",
    "    print(f\"Added {count} words to embedding\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "abeab4c80d6829cf2eae706bfa7929e2871af81f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "mispell_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3c09d981ae674e6a373189a04dba8d0932b0765b"
   },
   "source": [
    "The following toxic words are extracted from insincere questions, 1384 words in total. Codes can be found at the bottom. (Appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "1f3a41642c796f07c3b1d760cfa5d27e8b75b0fa"
   },
   "outputs": [],
   "source": [
    "real_toxic = ['soetoro', 'yall', 'islamaphob', 'usur', 'wanker', 'thole', 'cocksuck', 'twat', 'remoan', 'hindian', 'khazari', \n",
    "              'terroristan', 'bootlick', 'boglin', 'lathi', 'wmaf', 'auvela', 'simpleton', 'kike', 'hongkonges', 'jewism', \n",
    "              'chutzpah', 'jewdar', 'zoophil', 'bozo', 'zarat', 'dirtbag', 'witchhunt', 'choicer', 'wagga', 'zoophilia', \n",
    "              'peoplekind', 'moeslim', 'eroupian', 'cishet', 'perki', 'bomp', 'wolyn', 'brimston', 'overjoy', 'metzitzah', \n",
    "              'andhbhakt', 'myeshia', 'fack', 'manoo', 'oompa', 'loompa', 'demcorat', 'quranist', 'outbr', 'kagan', 'hylton', \n",
    "              'repub', 'hissi', 'teehe', 'sugarcoat', 'srkian', 'blowhard', 'trumpanze', 'caucas', 'kamagra', 'derrier', 'hahahah', \n",
    "              'largess', 'overproud', 'sugarbab', 'islamaphobia', 'armalit', 'neckbeard', 'trollbot', 'bonehead', 'insest', 'yanukovych', \n",
    "              'hubbel', 'lgbti', 'lgbqt', 'biid', 'fruitcak', 'brassier', 'clemenc', 'awlaki', 'perv', 'khanz', 'idolatr', 'turkifi', \n",
    "              'baiter', 'hinduphobia', 'jeran', 'witold', 'henpeck', 'hypocraci', 'ezor', 'hiraba', 'cuckserv', 'zoosad', 'iqer', \n",
    "              'mangalorean', 'bhapa', 'rapaci', 'radh', 'islaam', 'homoerot', 'phedophil', 'apeshit', 'incl', 'mugger', 'snivel', \n",
    "              'massler', 'undiplomat', 'dirtnap', 'turkist', 'castat', 'abyssinian', 'espinanzo', 'skkim', 'copout', 'japheth', \n",
    "              'natzi', 'gullabl', 'fauxcahonta', 'gobbi', 'putrid', 'nonflatt', 'ramgarhia', 'overset', 'underset', 'golddigg', \n",
    "              'pugb', 'womyn', 'shaniqua', 'homeboy', 'sotomayor', 'unemancip', 'tinni', 'torward', 'grandstand', 'vori', 'anglophob', \n",
    "              'coolaid', 'mafioso', 'facetard', 'fucki', 'puneet', 'gloryhol', 'lbgtq', 'bogan', 'soveriegn', 'smartia', 'dumbassistan', \n",
    "              'condoleezza', 'headstrong', 'predilect', 'motherfuckin', 'pinko', 'sincerest', 'allreadi', 'vartheta', 'asshat', 'sodomit', 'bremoan', 'reproach', 'razan', 'vrindavani', 'marinov', 'ghaati', 'gangaj', 'bukura', 'houseboy', 'discust', 'hoser', 'qouran', 'counterexampl', 'bordello', 'sengar', 'weeni', 'pussifi', 'dumbfuck', 'mungu', 'dravidanadu', 'rapistan', 'reappropri', 'senselessli', 'pjak', 'elephantin', 'shithead', 'clawn', 'bikya', 'masr', 'senousa', 'kahala', 'corduta', 'griot', 'womankind', 'lgbtqai', 'bhakth', 'refferandum', 'hahahaha', 'manifort', 'dhimmi', 'bludger', 'sephardim', 'remini', 'minutia', 'nauseum', 'effigi', 'shimon', 'pere', 'fillipino', 'majoosi', 'releasethememo', 'mahtob', 'niggl', 'helpag', 'disproport', 'honnavar', 'supermoon', 'mailmen', 'wahhabist', 'bukkari', 'apostat', 'fingur', 'crabbi', 'rohingnya', 'ruffian', 'madridiot', 'rastrakuta', 'revealingli', 'houri', 'clusterfuck', 'ruinat', 'niggeriah', 'nonchristian', 'wussi', 'vishnuist', 'pooper', 'tattl', 'premanand', 'biphobia', 'iraqui', 'heartlessli', 'discrim', 'sycophant', 'suduc', 'wacko', 'babchenko', 'scrimgeour', 'simper', 'choot', 'generalissimo', 'lgtbq', 'onerror', 'jamil', 'demonrat', 'uygur', 'courteous', 'ignoramu', 'lavon', 'banyak', 'hypercet', 'bhramin', 'dilit', 'whorish', 'girasa', 'genoves', 'uniron', 'libidin', 'awaya', 'demontis', 'stephanopoulo', 'nidal', 'jewconomi', 'respectful', 'kashmirian', 'electri', 'kangz', 'muslims', 'jayda', 'fransen', 'otim', 'mussi', 'gayish', 'hispano', 'frederica', 'rabidli', 'yigal', 'goodlatt', 'witless', 'unashamedli', 'donmeh', 'shimanski', 'idnani', 'devar', 'irfaan', 'ringlead', 'gayism', 'stelter', 'salonika', 'lockstep', 'sexili', 'bancoop', 'uttp', 'gleefulli', 'rapefuge', 'neic', 'magoo', 'katyn', 'mischaracter', 'humdrum', 'bukth', 'japhet', 'podunk', 'circlejerk', 'deicid', 'arkansaw', 'charedi', 'jewplic', 'barracoon', 'shirki', 'dacoiti', 'anglosaxon', 'statian', 'doklan', 'amrendra', 'thang', 'ovadia', 'souad', 'sheesh', 'eever', 'thuggish', 'troglodyt', 'docto', 'dickhol', 'mangina', 'goru', 'actually', 'hinduphob', 'blazingli', 'tooter', 'disengenu', 'boshniak', 'middleclass', 'penish', 'libturd', 'bridgett', 'muslm', 'trumpton', 'chodu', 'wonen', 'chup', 'pakistaini', 'mesta', 'browner', 'overgeneralis', 'wellkom', 'tbey', 'illicitli', 'pedog', 'trumptard', 'bronycon', 'jerkoff', 'unaesthet', 'demosthen', 'waggott', 'murda', 'kaci', 'strozk', 'scandanavian', 'sleepin', 'shiff', 'yobbo', 'liberachi', 'saveg', 'kardasian', 'anglocentr', 'lumberyard', 'esoterica', 'narikoravar', 'loudmouth', 'vagena', 'hilata', 'faantasi', 'schlong', 'maliyali', 'alari', 'braincel', 'eyerollingli', 'ophir', 'muta', 'radhswaomi', 'liberal', 'hypocrisy', 'shemen', 'hemen', 'thatwhi', 'bitingli', 'islamophilia', 'ayodya', 'ilegali', 'farrakan', 'anzakistan', 'knifer', 'scudus', 'sychopant', 'chaibala', 'achh', 'bisaya', 'trigglypuff', 'bakrichod', 'wellassa', 'fichteschen', 'schellingschen', 'shitslam', 'pakhandi', 'ejewc', 'islamif', 'djihaddist', 'freakout', 'suckhol', 'immutur', 'khatriya', 'rituel', 'lakhta', 'ociopath', 'dieudonn', 'discremin', 'atlst', 'tamilzan', 'busload', 'reasorc', 'shitheel', 'nchausen', 'pampuia', 'fiuta', 'ebba', 'putang', 'douchebagi', 'odonnel', 'climateg', 'matherfuck', 'dysfuntin', 'keylor', 'infantino', 'nopenmur', 'oftheir', 'evangilitac', 'bussca', 'moplah', 'whinehous', 'prejust', 'illiteratendra', 'feminim', 'infedel', 'barmaid', 'adharsh', 'betamal', 'swatt', 'walding', 'mudbon', 'spentbtim', 'butterfac', 'liberalaphobia', 'shara', 'turanist', 'laner', 'harrem', 'gonen', 'segev', 'andressen', 'crocdil', 'maharahtra', 'weener', 'lesin', 'pridnestrovi', 'lensmak', 'sccount', 'thereproblem', 'whiet', 'spanke', 'leakendra', 'zuraida', 'kamaruddin', 'dipshit', 'scummer', 'screqe', 'hezbolla', 'extortion', 'cuckholdri', 'heyberi', 'secund', 'mahapow', 'bsbe', 'punjaban', 'playth', 'transgener', 'whitelash', 'usele', 'liliettedo', 'matteer', 'raceism', 'plonker', 'khaleeji', 'deerfield', 'klansmen', 'anymore', 'ladan', 'milionair', 'minsit', 'externalist', 'feto', 'singlepay', 'adoles', 'titfuck', 'tennesseu', 'khurmatu', 'grouper', 'rammayana', 'tkink', 'gurdaa', 'insaan', 'carasso', 'jabotinski', 'parvu', 'roddenberri', 'ashyayyyyy', 'clownish', 'modichod', 'ahilya', 'sherif', 'greedo', 'genetilia', 'mengrelian', 'bashirhat', 'causian', 'otherw', 'soyboy', 'unspeci', 'catestroph', 'burek', 'vagimir', 'poontang', 'rafidhi', 'stinkin', 'motti', 'stoooopid', 'enjiy', 'heim', 'paskitan', 'lieng', 'maffia', 'absentminded', 'naredra', 'retitl', 'saygin', 'yalcin', 'patita', 'exonor', 'yogiji', 'buttsor', 'territorist', 'pedophillo', 'prosucut', 'egsist', 'springstein', 'mifsud', 'donate', 'painti', 'cruellest', 'ugenasist', 'ketevan', 'lgbtqqiaap', 'pangend', 'ginorm', 'biddi', 'incestophil', 'gandl', 'reinfectu', 'zakia', 'belkhiri', 'kulak', 'whypakistan', 'sleazebal', 'shanaya', 'wringli', 'unicivlis', 'perforc', 'ivanca', 'clop', 'phylosaphi', 'coocoo', 'leecher', 'neaderthal', 'tastelessli', 'libera', 'hackwork', 'karandlaj', 'manlyhood', 'chaft', 'zooland', 'bechet', 'omnibook', 'projectmakom', 'practiv', 'thammana', 'serapi', 'christu', 'junkook', 'inshirian', 'kejru', 'puddingpop', 'fyromian', 'unputin', 'aligatt', 'neonazi', 'domex', 'bachchabazi', 'hunker', 'yoshiro', 'multicur', 'venganc', 'steni', 'pepplr', 'julkar', 'khate', 'pasttim', 'middleschool', 'handsmaid', 'tembl', 'dimbl', 'mapilla', 'estabilish', 'baluchi', 'gleeful', 'gawp', 'sinia', 'chinnes', 'peadofilia', 'alaria', 'velayat', 'faqih', 'bastardi', 'abney', 'satti', 'fatherboard', 'parentboard', 'duetert', 'penatra', 'fantiz', 'urbanit', 'cunning', 'cprf', 'meer', 'ancest', 'bafoon', 'thaman', 'zeme', 'dicksuck', 'tiannanmen', 'shaub', 'abhineet', 'iryna', 'bilyk', 'practc', 'reprocuss', 'hardick', 'canuk', 'vulgur', 'obsessed', 'nympho', 'dyfunt', 'badmoth', 'flippin', 'smeel', 'langkian', 'paquin', 'single', 'jewlet', 'palestanian', 'beyter', 'dustruct', 'websight', 'whola', 'baffoon', 'extrimisit', 'hjmni', 'bgettfbrfvsbtrfvnhtgfvtnh', 'rbghnrjthfsgrnhtf', 'bjhgfdv', 'yjvd', 'bbmmjijikjki', 'kkiuipuyffgfff', 'yuyhjun', 'mkijthfnymufynhgb', 'indissolubl', 'flurish', 'athieth', 'inferiort', 'briitsh', 'comonwealth', 'fakestinian', 'cacusian', 'privalg', 'extratestri', 'mooslim', 'trumpin', 'rathar', 'ladhak', 'watchabl', 'gaudiya', 'madresa', 'mahashivratri', 'discreetstor', 'aknowledg', 'loor', 'sphani', 'causs', 'hahahahahahah', 'starin', 'boehner', 'fullstop', 'spose', 'yufeng', 'trumpito', 'gamerg', 'ayman', 'mohyeldin', 'propalestinian', 'pointedli', 'ferozepur', 'dtermin', 'chhatrapathi', 'europen', 'mcgucken', 'mapabl', 'cybertrol', 'braggi', 'ccaus', 'jacker', 'dominionist', 'religeon', 'plumpi', 'crudup', 'magazine', 'paigambar', 'twaught', 'baneerje', 'iolanda', 'blee', 'hardhead', 'mudsling', 'whatabouteri', 'karaiyan', 'straightfoward', 'honseijin', 'regino', 'snowflakey', 'becoming', 'ilik', 'diphteria', 'apostatis', 'jewocraci', 'managementskil', 'seacoast', 'pederast', 'lefist', 'teluguwala', 'chinkistan', 'electiom', 'askhenazi', 'hartal', 'oppres', 'haro', 'preas', 'antyth', 'twirk', 'offa', 'indevu', 'inproportion', 'pthan', 'whabi', 'cilit', 'helsiinki', 'shyt', 'navaj', 'besmirch', 'holocuast', 'nometri', 'hynd', 'shopian', 'sleazebag', 'edglelord', 'lingchi', 'virginit', 'demacrat', 'sigint', 'priiz', 'freakier', 'causinng', 'falangist', 'sexuality', 'leeland', 'shrimpboy', 'tepperberg', 'untermensch', 'dikh', 'pappulog', 'ducj', 'audiancecy', 'ifcso', 'drawl', 'outfought', 'coldheart', 'soreceri', 'superunif', 'expatriot', 'othet', 'demcrat', 'monard', 'longmont', 'eyehol', 'kapolei', 'greasier', 'declassif', 'smush', 'illeagl', 'bretheren', 'izaq', 'rendia', 'kangladesh', 'necermind', 'parda', 'trudea', 'dalyellup', 'cambat', 'congoid', 'asiasoid', 'chandrashekhar', 'lesbiam', 'abeshia', 'terorrist', 'presbyt', 'narandar', 'precariat', 'antihindu', 'antijew', 'gursikh', 'nonpract', 'wirshipp', 'juar', 'zucc', 'azerbajani', 'bullycid', 'jait', 'deepti', 'terrror', 'lezbien', 'rentomojo', 'ifugao', 'narzil', 'afriforum', 'gladden', 'breathian', 'kenedi', 'kallstrom', 'coonserv', 'rajmandir', 'prorogu', 'womais', 'stickup', 'feadup', 'cakemak', 'saffronis', 'sportiv', 'murican', 'pakodaman', 'insterburg', 'memel', 'allenstein', 'pillau', 'helisberg', 'gumbinnen', 'nordenburg', 'kolberg', 'sabhi', 'gyaan', 'baatn', 'emptier', 'shagger', 'coroplast', 'worshop', 'hinsu', 'dumbasseri', 'machedeo', 'doofus', 'beauto', 'wohn', 'kurk', 'donno', 'deletequora', 'caucho', 'gook', 'saipul', 'turnendra', 'ahich', 'tinoo', 'foolest', 'informatio', 'fedoff', 'atttack', 'overcompens', 'politicain', 'miquel', 'sirvent', 'frego', 'talaat', 'bettina', 'arndt', 'ivorian', 'shouldjudiciari', 'preodenti', 'smoochi', 'inshalol', 'deptart', 'jailtim', 'condenscend', 'scroung', 'karsevak', 'laxit', 'theirselev', 'uzbekistani', 'blubd', 'rosneft', 'mindfuckin', 'thyey', 'hijo', 'puta', 'faced', 'kenpeitai', 'thaanai', 'thalaivar', 'untrol', 'janitch', 'iinspit', 'coprophagia', 'witchunt', 'mangoliod', 'bmlm', 'burret', 'anthraci', 'unballanc', 'jewsplain', 'hindni', 'qaum', 'opset', 'tormoil', 'overprivileg', 'jeisu', 'winded', 'disagreemen', 'paskistani', 'gynocentr', 'sittin', 'antifascist', 'beuati', 'parashar', 'annihal', 'mcjew', 'jewovitch', 'jewski', 'ninesensical', 'americocentr', 'rezeki', 'agaw', 'squanch', 'barreto', 'haffiz', 'syeed', 'mizoz', 'godsak', 'dhimmitud', 'mynmar', 'superho','thappad', 'tamacha', 'chooseday', 'fauti', 'doctuh', 'christineti', 'brexitosi', 'behenchod', 'kopelman', 'pusdi', 'binyamin', 'reshapen', 'cockhead', 'baldlos', 'antilgbtq', 'untalk', 'mcanal', 'hugenont', 'reguge', 'mouthof', 'rubbishi', 'nthe', 'pissyleak', 'engllish', 'carnist', 'awhit', 'malkin', 'receb', 'amillion', 'brazillan', 'khandhan', 'unequiv', 'niga', 'titsfuck', 'femwhin', 'varpi', 'maranzano', 'profaci', 'gagliano', 'dickstein', 'bareback', 'dahej', 'drund', 'schlussel', 'sceced', 'khortha', 'howmoth', 'unabashedli', 'demetrio', 'sonscritona', 'sikhim', 'pcoptimum', 'monther', 'cockoroach', 'hissyfit', 'pooran', 'fugli', 'halakha', 'gurl', 'newsclip', 'mahahatma', 'islamic', 'hostpit', 'choota', 'haterd', 'resourceless', 'narcissistici', 'errog', 'jalikatu', 'zzzzzz', 'sapless', 'organix', 'namessak', 'sisterfuck', 'demigaug', 'misshap', 'blubberi', 'frienemi', 'anuwind', 'prognath', 'slimebal', 'ditheist', 'mmel', 'dought', 'mongolianz', 'blurr', 'racewar', 'peadopbil', 'musevi', 'assassinated', 'woebegon', 'bonespur', 'migl', 'chilenean', 'khadem', 'oblongata', 'japhetit', 'formosan', 'waali', 'baai', 'arundati', 'suliman', 'masterbur', 'dumbocrat', 'bungler', 'chakka', 'eunech', 'adrianna', 'barbeau', 'striaght', 'uwhi', 'cointelpro', 'witr', 'pinay', 'skipral', 'adjudg', 'expositor', 'whiteboy', 'boorstein', 'radhik', 'toastsexu', 'evononsens', 'rafidha', 'affan', 'intermesh', 'sunhy', 'raciou', 'fairytail', 'grettel', 'chiptun', 'critics', 'mainstem', 'commons', 'vikern', 'coudnt', 'alphagob', 'codeword', 'toiletpap', 'freemasin', 'kashmi', 'papalist', 'yechuri', 'distabilis', 'royspearsblog', 'mughrabi', 'fvcking', 'arund', 'chineses', 'doodoo', 'profetacid', 'uhmmm', 'abcdefghijklmnopqrstuvwxyz', 'badruddin', 'ajmal', 'buysoma', 'shexian', 'gosch', 'assett', 'promuslim', 'femisint', 'reaccept', 'interjector', 'midianit', 'ickiest', 'msot', 'puntuc', 'intactivist', 'agirl', 'pantyless', 'eneyon', 'bullyboy', 'nomi', 'muslimophobia', 'fleed', 'postbellum', 'transgenu', 'purgegam', 'votr', 'gudi', 'padwa', 'bloodfart', 'muton', 'ugggggggllli', 'trumpid', 'dixieland', 'safegenericpharmaci', 'comradeship', 'avalur', 'frigin', 'khalq', 'grandli', 'seduic', 'countrywomen', 'testicles', 'erop', 'convanc', 'littledyck', 'ricist', 'flurazepam', 'unjoin', 'nunesmemo', 'govshutdown', 'trumpli', 'lonley', 'electrogravit', 'antireligion', 'fornicar', 'inexcus', 'brahui', 'sepratist', 'guiltier', 'footbind', 'senomyx', 'dommi', 'gaurang', 'oslash', 'whatshi', 'ticrapo', 'unwsnt', 'afrucan', 'nutter', 'ncrypto', 'janakpur', 'wheee', 'demecrat', 'videogamedunki', 'dunki', 'kanhaya', 'crufic', 'unclassi', 'poojari', 'fervenc', 'paelo', 'fujianes', 'shoney', 'buereaucrat', 'antiamerican', 'squirmi', 'snakelik', 'hona', 'geftaman', 'milliband', 'indish', 'resound', 'bagdadi', 'unpeac', 'spitter', 'hiten', 'mannish', 'relevantli', 'reinvestigst', 'shrinidhi', 'douchey', 'ocasion', 'eassili', 'ameri', 'mcmuffin', 'practicli', 'nuuk', 'castoff', 'oveckin', 'aremenian', 'communalist', 'whereupon', 'biaexit', 'alquada', 'comfot', 'monomaniac', 'funcaptcha', 'dignidad', 'demascul', 'unbelief', 'pedowood', 'antigunn', 'vryant', 'equalistar', 'ovedu', 'blakc', 'lebian', 'bluewat', 'musilim', 'qstn', 'gordita', 'wowowin', 'masturbatori', 'fuckn', 'muahmmad', 'armostrax', 'pregegn', 'uchranian', 'bhuttani', 'intercors', 'sympathes', 'altai', 'lickiti', 'tamirlarg', 'tamirlnadu', 'kalshnikov', 'dumpsit', 'murtadeen', 'seehof', 'progressivist', 'guliani', 'hizra', 'copulatori', 'husoon', 'vegana', 'homosoci', 'defecat', 'diaoyudao', 'populum', 'selll', 'dervish', 'fashist', 'fader', 'tukd', 'tudk', 'baffoonish', 'sutpid', 'junagarh', 'xingp', 'shaitan', 'coatian', 'patmo', 'jaichand', 'ferrog', 'bholeje', 'mataje', 'hsun', 'rewrot', 'whyall', 'sanyasa', 'khoja', 'ahmadia', 'farcism', 'beefburg', 'clothphemi', 'repsect', 'sarvjeet', 'bumiputera', 'gether', 'scamgress', 'genda', 'huehuehuehuehu', 'cnnblackmail', 'feliformia', 'cadsoft', 'hypocritic', 'barzilai', 'shivdharma', 'guesswork', 'atlanticist', 'chitragong', 'fliud', 'croch', 'haastrup', 'mccray', 'taurus', 'undrip', 'subbotonik', 'sharadevi', 'christianphob', 'lives', 'dusfunt', 'acrit', 'asswhol', 'defiantli', 'diahhrea', 'bitransgend', 'dhesiyam', 'malcont', 'ausi', 'profiel', 'daco', 'flippant', 'agitprop', 'jockin', 'gunderson', 'sportswomen', 'trangend', 'obamaand', 'eeewww', 'mmmmm', 'sodomis', 'bitc', 'piehol', 'kanglu', 'conmen', 'manichaeist', 'honorkil', 'amongua', 'argentian', 'kalapani', 'gruffer', 'bhaichung', 'bhutia', 'transfung', 'eveb', 'comf', 'reaganist', 'napali', 'masshol', 'anthropophagolagnia', 'dharmik']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "63cb21525251b060aeb309e7be4b48772f8720f5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def add_features(df):\n",
    "    \n",
    "    df['question_text'] = df['question_text'].progress_apply(lambda x:str(x))\n",
    "    df['total_length'] = df['question_text'].progress_apply(len)\n",
    "    df['capitals'] = df['question_text'].progress_apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.progress_apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    df['num_words'] = df.question_text.str.count('\\S+')\n",
    "    df['num_unique_words'] = df['question_text'].progress_apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']\n",
    "    \n",
    "    def toxic_word_ct(txt):\n",
    "        ct = 0\n",
    "        toxic_word = ['liber','democra','jew','muslim','white','gay','hindu','trump','women','black','christian','hate']\n",
    "        for word in toxic_word:\n",
    "            if word in txt:\n",
    "                ct=1\n",
    "                break\n",
    "        return ct\n",
    "    \n",
    "    def unique_toxic(txt):\n",
    "        ct = 0\n",
    "        toxic_word = real_toxic\n",
    "        for word in toxic_word:\n",
    "            if word in txt:\n",
    "                ct=1\n",
    "                break\n",
    "        return ct\n",
    "            \n",
    "    df['sensetive'] = df['question_text'].progress_apply(toxic_word_ct)\n",
    "    df['unique_tx'] = df['question_text'].progress_apply(unique_toxic)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_and_prec():\n",
    "    train_df = pd.read_csv(\"../input/train.csv\")\n",
    "    test_df = pd.read_csv(\"../input/test.csv\")\n",
    "    print(\"Train shape : \",train_df.shape)\n",
    "    print(\"Test shape : \",test_df.shape)\n",
    "    \n",
    "    # Lower\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "\n",
    "    # Clean the text\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
    "    \n",
    "    # Clean numbers\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "    \n",
    "    # Clean speelings\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "    \n",
    "    # Fill up the missing values\n",
    "    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n",
    "    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n",
    "\n",
    "    \n",
    "    # Add features\n",
    "    train = add_features(train_df)\n",
    "    test = add_features(test_df)\n",
    "\n",
    "    features = train[['caps_vs_length', 'words_vs_unique','sensetive','unique_tx','total_length','capitals','num_words','num_unique_words']].fillna(0)\n",
    "    test_features = test[['caps_vs_length', 'words_vs_unique','sensetive','unique_tx','total_length','capitals','num_words','num_unique_words']].fillna(0)\n",
    "    \n",
    "    # Standardize those features\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(np.vstack((features, test_features)))\n",
    "    features = ss.transform(features)\n",
    "    test_features = ss.transform(test_features)\n",
    "\n",
    "    # Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "    # Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "    # Get the target values\n",
    "    train_y = train_df['target'].values\n",
    "    \n",
    "    # Shuffling the data\n",
    "    np.random.seed(SEED)\n",
    "    trn_idx = np.random.permutation(len(train_X))\n",
    "\n",
    "    train_X = train_X[trn_idx]\n",
    "    train_y = train_y[trn_idx]\n",
    "    features = features[trn_idx]\n",
    "    \n",
    "    return train_X, test_X, train_y, features, test_features, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "3c72fcddb4f680879e231c3dbfc0c71e27fc424c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (375806, 2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba406a92f344a76bc9fdb0e63351f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79dd51aee5e41b3a8ebb01fbbae8914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=375806, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7312b27c48bb4cefbcc25112a2d1bd59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe97b4188584879bc28c842baf9234c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=375806, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b92b5ee64e45ec8de6d19c5c43efd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c49799d9830470887fc30e4b969b9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e776085f6e6d45b794a83f13ba5dd05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b04a0378ebb4ee5ad6042ef40949305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1b08fb05c7412bbf9953e8dc2342db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f591af91d4e4451b9a22096fd4dca30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0db2b4855a4aa99d35efd35029e3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cddff8faa44617b749014deb7eb188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=375806, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d751d10df254c1da609dde38a4f9519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=375806, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8071f7f3fb954467add59d51408c549f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=375806, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2032acd526dd4d6fa35420e10d61840f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=375806, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, x_test, y_train, features, test_features, word_index = load_and_prec() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e5c51a8329d569d13b9f0369ebb98ca8e2e55440"
   },
   "source": [
    "### Load Embeddings\n",
    "\n",
    "Two embedding matrices have been used. Glove, and paragram. The mean of the two is used as the final embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "6a5f4502324d369ff6faa3692accee4f8a233005",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:49: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(120000, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing entries in the embedding are set using np.random.normal so we have to seed here too\n",
    "seed_everything()\n",
    "\n",
    "glove_embeddings = load_glove(word_index)\n",
    "paragram_embeddings = load_para(word_index)\n",
    "# fasttext_embeddings = load_fasttext(word_index)\n",
    "\n",
    "embedding_matrix = np.mean([glove_embeddings, paragram_embeddings], axis=0)\n",
    "embedding_matrix_cat = np.concatenate((glove_embeddings, paragram_embeddings), axis=1)\n",
    "# vocab = build_vocab(df['question_text'])\n",
    "# add_lower(embedding_matrix, vocab)\n",
    "# del glove_embeddings, paragram_embeddings, fasttext_embeddings\n",
    "del glove_embeddings, paragram_embeddings\n",
    "gc.collect()\n",
    "\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3b707858357eec18403f97368a40c43987b40e8"
   },
   "source": [
    "# Modeling\n",
    "\n",
    "For model diversity, five various architectures are used. Also, since the setting in pytorch and keras are different, it would be interesting to take language into account when considering diversity.\n",
    "- Pytorch:\n",
    "    - LSTM+GRU+CapsNet\n",
    "- Keras:\n",
    "    - GRU+2Poolings \n",
    "    - LSTM+Attention+2Poolings\n",
    "    - CNN with filter_size = 3,4,5,10\n",
    "    - GRU+CapsNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8098bea0cee9117ff9dc4e11feba53e49b80cb55"
   },
   "source": [
    "## Pytorch Model\n",
    "### Cyclic CLR\n",
    "Code taken from https://www.kaggle.com/dannykliu/lstm-with-attention-clr-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "9d531a7454923f90d0e7443b1ed1373d008c2e88",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code inspired from: https://github.com/anandsaha/pytorch.cyclic.learning.rate/blob/master/cls.py\n",
    "class CyclicLR(object):\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b8caecb207a12d4a5524fe16e4524b31c7da8bac"
   },
   "source": [
    "### Model Architecture\n",
    "\n",
    "Binary LSTM with an attention layer and an additional fully connected layer. Also added extra features taken from a winning kernel of the toxic comments competition. Also using CLR and a capsule Layer. Blended together in concatentation.\n",
    "\n",
    "Initial idea borrowed from: https://www.kaggle.com/ziliwang/baseline-pytorch-bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "8f7e1d15451201efbac2741b03d4b0dfd3b9bfe0"
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "embedding_dim = 300\n",
    "embedding_path = '../save/embedding_matrix.npy'  # or False, not use pre-trained-matrix\n",
    "use_pretrained_embedding = True\n",
    "\n",
    "hidden_size = 64\n",
    "gru_len = hidden_size\n",
    "\n",
    "Routings = 4 #5\n",
    "Num_capsule = 5\n",
    "Dim_capsule = 5#16\n",
    "dropout_p = 0.25\n",
    "rate_drop_dense = 0.28\n",
    "LR = 0.001\n",
    "T_epsilon = 1e-7\n",
    "num_classes = 30\n",
    "\n",
    "\n",
    "class Embed_Layer(nn.Module):\n",
    "    def __init__(self, embedding_matrix=None, vocab_size=None, embedding_dim=300):\n",
    "        super(Embed_Layer, self).__init__()\n",
    "        self.encoder = nn.Embedding(vocab_size + 1, embedding_dim)\n",
    "        if use_pretrained_embedding:\n",
    "            # self.encoder.weight.data.copy_(t.from_numpy(np.load(embedding_path))) # 方法一，加载np.save的npy文件\n",
    "            self.encoder.weight.data.copy_(t.from_numpy(embedding_matrix))  # 方法二\n",
    "\n",
    "    def forward(self, x, dropout_p=0.25):\n",
    "        return nn.Dropout(p=dropout_p)(self.encoder(x))\n",
    "\n",
    "\n",
    "class GRU_Layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRU_Layer, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=300,\n",
    "                          hidden_size=gru_len,\n",
    "                          bidirectional=True)\n",
    "\n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gru(x)\n",
    "\n",
    "\n",
    "# core caps_layer with squash func\n",
    "class Caps_Layer(nn.Module):\n",
    "    def __init__(self, input_dim_capsule=gru_len * 2, num_capsule=Num_capsule, dim_capsule=Dim_capsule, \\\n",
    "                 routings=Routings, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Caps_Layer, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size  # 暂时没用到\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = self.squash\n",
    "        else:\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "        if self.share_weights:\n",
    "            self.W = nn.Parameter(\n",
    "                nn.init.xavier_normal_(t.empty(1, input_dim_capsule, self.num_capsule * self.dim_capsule)))\n",
    "        else:\n",
    "            self.W = nn.Parameter(\n",
    "                t.randn(BATCH_SIZE, input_dim_capsule, self.num_capsule * self.dim_capsule))  # 64即batch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = t.matmul(x, self.W)\n",
    "        else:\n",
    "            print('add later')\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        input_num_capsule = x.size(1)\n",
    "        u_hat_vecs = u_hat_vecs.view((batch_size, input_num_capsule,\n",
    "                                      self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = u_hat_vecs.permute(0, 2, 1, 3)  # change into(batch_size,num_capsule,input_num_capsule,dim_capsule)\n",
    "        b = t.zeros_like(u_hat_vecs[:, :, :, 0])  # (batch_size,num_capsule,input_num_capsule)\n",
    "\n",
    "        for i in range(self.routings):\n",
    "            b = b.permute(0, 2, 1)\n",
    "            c = F.softmax(b, dim=2)\n",
    "            c = c.permute(0, 2, 1)\n",
    "            b = b.permute(0, 2, 1)\n",
    "            outputs = self.activation(t.einsum('bij,bijk->bik', (c, u_hat_vecs)))  # batch matrix multiplication\n",
    "            # outputs shape (batch_size, num_capsule, dim_capsule)\n",
    "            if i < self.routings - 1:\n",
    "                b = t.einsum('bik,bijk->bij', (outputs, u_hat_vecs))  # batch matrix multiplication\n",
    "        return outputs  # (batch_size, num_capsule, dim_capsule)\n",
    "\n",
    "    # text version of squash, slight different from original one\n",
    "    def squash(self, x, axis=-1):\n",
    "        s_squared_norm = (x ** 2).sum(axis, keepdim=True)\n",
    "        scale = t.sqrt(s_squared_norm + T_epsilon)\n",
    "        return x / scale\n",
    "    \n",
    "class Capsule_Main(nn.Module):\n",
    "    def __init__(self, embedding_matrix=None, vocab_size=None):\n",
    "        super(Capsule_Main, self).__init__()\n",
    "        self.embed_layer = Embed_Layer(embedding_matrix, vocab_size)\n",
    "        self.gru_layer = GRU_Layer()\n",
    "        # important (initalization)\n",
    "        self.gru_layer.init_weights()\n",
    "        self.caps_layer = Caps_Layer()\n",
    "        self.dense_layer = Dense_Layer()\n",
    "\n",
    "    def forward(self, content):\n",
    "        content1 = self.embed_layer(content)\n",
    "        content2, _ = self.gru_layer(content1)  # output(seq_len, batch_size, num_directions * hidden_size)，and hn\n",
    "        content3 = self.caps_layer(content2)\n",
    "        output = self.dense_layer(content3)\n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "170b27f66c7e546d8e84c79357d40f86c6b1ec42",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        fc_layer = 16\n",
    "        fc_layer1 = 16\n",
    "\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.lstm_attention = Attention(hidden_size * 2, maxlen)\n",
    "        self.gru_attention = Attention(hidden_size * 2, maxlen)\n",
    "        self.bn = nn.BatchNorm1d(16, momentum=0.5)\n",
    "        self.linear = nn.Linear(hidden_size*8+3, fc_layer1) #643:80 - 483:60 - 323:40\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(fc_layer**2,fc_layer)\n",
    "        self.out = nn.Linear(fc_layer, 1)\n",
    "        self.lincaps = nn.Linear(Num_capsule * Dim_capsule, 1)\n",
    "        self.caps_layer = Caps_Layer()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        h_embedding = self.embedding(x[0])\n",
    "        h_embedding = torch.squeeze(\n",
    "            self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
    "        \n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "\n",
    "        ##Capsule Layer        \n",
    "        content3 = self.caps_layer(h_gru)\n",
    "        content3 = self.dropout(content3)\n",
    "        batch_size = content3.size(0)\n",
    "        content3 = content3.view(batch_size, -1)\n",
    "        content3 = self.relu(self.lincaps(content3))\n",
    "\n",
    "        ##Attention Layer\n",
    "        h_lstm_atten = self.lstm_attention(h_lstm)\n",
    "        h_gru_atten = self.gru_attention(h_gru)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_gru, 1)\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_gru, 1)\n",
    "        \n",
    "        f = torch.tensor(x[1], dtype=torch.float).cuda()\n",
    "\n",
    "                #[512,160]\n",
    "        conc = torch.cat((h_lstm_atten, h_gru_atten,content3, avg_pool, max_pool,f), 1)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.bn(conc)\n",
    "        conc = self.dropout(conc)\n",
    "\n",
    "        out = self.out(conc)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4e47597cde552a41cdd8ec2531aa6a861e491ae"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "704fcb82f5f25d6349a487409f519384df352375"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.dataset[index]\n",
    "\n",
    "        return data, target, index\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "0c6339acdc14e6688ed47e9f439e81cd7cfca57f"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# matrix for the out-of-fold predictions\n",
    "train_preds = np.zeros((len(x_train)))\n",
    "# matrix for the predictions on the test set\n",
    "test_preds = np.zeros((len(df_test)))\n",
    "\n",
    "# always call this before training for deterministic results\n",
    "seed_everything()\n",
    "\n",
    "x_test_cuda = torch.tensor(x_test, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "avg_losses_f = []\n",
    "avg_val_losses_f = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "2cfdd46b117a11af54ed01f1f9511782e50a00fa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint2\n",
      "Epoch 1/5 \t loss=72.8270 \t val_loss=56.9911 \t time=368.02s\n",
      "checkpoint1\n",
      "checkpoint2\n",
      "Epoch 2/5 \t loss=56.9855 \t val_loss=59.4770 \t time=369.26s\n",
      "checkpoint1\n",
      "checkpoint2\n",
      "Epoch 3/5 \t loss=53.6386 \t val_loss=44.1465 \t time=368.94s\n",
      "checkpoint1\n",
      "checkpoint2\n",
      "Epoch 4/5 \t loss=50.9223 \t val_loss=38.0433 \t time=368.41s\n",
      "checkpoint1\n",
      "checkpoint2\n",
      "Epoch 5/5 \t loss=48.0601 \t val_loss=38.1754 \t time=369.48s\n",
      "All \t loss=48.0601 \t val_loss=38.1754 \t \n"
     ]
    }
   ],
   "source": [
    "# split data in train / validation according to the KFold indeces\n",
    "# also, convert them to a torch tensor and store them on the GPU (done with .cuda())\n",
    "\n",
    "x_train_fold = torch.tensor(x_train, dtype=torch.long).cuda()\n",
    "y_train_fold = torch.tensor(y_train[:, np.newaxis], dtype=torch.float32).cuda()\n",
    "\n",
    "kfold_X_features = features[:,[0,1]]\n",
    "kfold_X_valid_features = features[:,[0,1]]\n",
    "x_val_fold = torch.tensor(x_train, dtype=torch.long).cuda()\n",
    "y_val_fold = torch.tensor(y_train[:, np.newaxis], dtype=torch.float32).cuda()\n",
    "\n",
    "model = NeuralNet()\n",
    "\n",
    "# make sure everything in the model is running on the GPU\n",
    "model.cuda()\n",
    "\n",
    "# define binary cross entropy loss\n",
    "# note that the model returns logit to take advantage of the log-sum-exp trick \n",
    "# for numerical stability in the loss\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "step_size = 300\n",
    "base_lr, max_lr = 0.001, 0.003   \n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                         lr=max_lr)\n",
    "\n",
    "################################################################################################\n",
    "scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,\n",
    "           step_size=step_size, mode='exp_range',\n",
    "           gamma=0.99994)\n",
    "###############################################################################################\n",
    "\n",
    "train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "\n",
    "train = MyDataset(train)\n",
    "valid = MyDataset(valid)\n",
    "\n",
    "##No need to shuffle the data again here. Shuffling happens when splitting for kfolds.\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# print(f'Fold {i + 1}')\n",
    "for epoch in range(n_epochs):\n",
    "    # set train mode of the model. This enables operations which are only applied during training like dropout\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "\n",
    "    avg_loss = 0.  \n",
    "    for i, (x_batch, y_batch, index) in enumerate(train_loader):\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        ################################################################################################            \n",
    "        f = kfold_X_features[index]\n",
    "        y_pred = model([x_batch,f])\n",
    "        ################################################################################################\n",
    "\n",
    "        ################################################################################################\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.batch_step()\n",
    "        ################################################################################################\n",
    "\n",
    "        # Compute and print loss.\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the Tensors it will update (which are the learnable weights\n",
    "        # of the model)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its parameters\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "    # set evaluation mode of the model. This disabled operations which are only applied during training like dropout\n",
    "    model.eval()\n",
    "\n",
    "    # predict all the samples in y_val_fold batch per batch\n",
    "    valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "    test_preds_fold = np.zeros((len(df_test)))\n",
    "\n",
    "    avg_val_loss = 0.\n",
    "    print('checkpoint1')\n",
    "    for i, (x_batch, y_batch, index) in enumerate(valid_loader):\n",
    "        f = kfold_X_valid_features[index]\n",
    "        y_pred = model([x_batch,f]).detach()\n",
    "\n",
    "        avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "    print('checkpoint2')\n",
    "    elapsed_time = time.time() - start_time \n",
    "    print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n",
    "        epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n",
    "    \n",
    "avg_losses_f.append(avg_loss)\n",
    "avg_val_losses_f.append(avg_val_loss) \n",
    "# predict all samples in the test set batch per batch\n",
    "for i, (x_batch,) in enumerate(test_loader):\n",
    "    f = test_features[i * batch_size:(i+1) * batch_size][:,[0,1]]\n",
    "    y_pred = model([x_batch,f]).detach()\n",
    "\n",
    "    test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "#     train_preds[valid_idx] = valid_preds_fold\n",
    "train_preds = valid_preds_fold\n",
    "# test_preds += test_preds_fold / len(splits)\n",
    "\n",
    "print('All \\t loss={:.4f} \\t val_loss={:.4f} \\t '.format(np.average(avg_losses_f),np.average(avg_val_losses_f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1d2187d4bbf48350eaf365b6dd8b027d5be69e0c"
   },
   "source": [
    "## Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import *\n",
    "import keras.backend as K\n",
    "from keras.callbacks import *\n",
    "import tensorflow as tf\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "dc4a4c681294ba06526fed4e871cfe8639cf25e4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'threshold': 0.24, 'f1': 0.7890822681670845}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in [i * 0.01 for i in range(100)]:\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result\n",
    "th_f1 = threshold_search(y_train,train_preds)\n",
    "print(th_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "d91aab5b6c512fc5906dd6d04a9cb832984d516e"
   },
   "outputs": [],
   "source": [
    "trainX = x_train; trainy = y_train; test_X = x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "31f62c41543c82b86eb3373c2b89fd700668a087"
   },
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    '''\n",
    "    metric from here \n",
    "    https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
    "    '''\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def train_pred(model, epochs=2,callback=None,fea=False):\n",
    "    for e in range(epochs):\n",
    "        if fea==False:\n",
    "            model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(val_X, val_y),callbacks=callback,verbose=1)\n",
    "            pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)\n",
    "        else:\n",
    "            model.fit([train_X,train_f], train_y, batch_size=512, epochs=1, validation_data=([val_X,val_f], val_y),callbacks=callback,verbose=1)\n",
    "            pred_val_y = model.predict([val_X,val_f], batch_size=1024, verbose=0)\n",
    "        \n",
    "        best_thresh = 0.5\n",
    "        best_score = 0.0\n",
    "        \n",
    "        for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "            thresh = np.round(thresh, 2)\n",
    "            score = f1_score(val_y, (pred_val_y > thresh).astype(int))\n",
    "            if score > best_score:\n",
    "                best_thresh = thresh\n",
    "                best_score = score\n",
    "        print(\"Epoch: \", e, \"-    best Val F1 Score: {:.4f}, best threshold: {:.4f}\".format(best_score,best_thresh))\n",
    "    report = classification_report(val_y, (pred_val_y > thresh).astype(int))\n",
    "    print('classification report:\\n',report)\n",
    "#     model.fit(trainX,trainy,batch_size=1024,epochs=1,verbose=1)\n",
    "    if fea==False:\n",
    "        pred_test_y = model.predict([test_X],batch_size=1024, verbose=0)\n",
    "    else:\n",
    "        pred_test_y = model.predict([test_X,test_features],batch_size=1024, verbose=0)\n",
    "    print('=' * 60)\n",
    "    return pred_val_y, pred_test_y, best_thresh,report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "e8d54e3386729011950cc4409cf5fc7a185fc730"
   },
   "outputs": [],
   "source": [
    "def model_gru_2pool(embedding_matrix):\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    fea = Input(shape=(8,), name='Features')\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    conc = concatenate([conc,fea])\n",
    "    conc = Dense(64, activation=\"relu\")(conc)\n",
    "#     conc = Dropout(0.1)(conc)\n",
    "    outp = Dense(1, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=[inp,fea], outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "83798d690fe6c07e7d3cf2d382e168814da47380"
   },
   "outputs": [],
   "source": [
    "def model_lstm_atten2(embedding_matrix):\n",
    "    \n",
    "    inp = Input(shape=(maxlen,))\n",
    "    \n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "    y = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "    \n",
    "    atten_1 = Attention(maxlen)(x) # skip connect\n",
    "    atten_2 = Attention(maxlen)(y)\n",
    "    avg_pool = GlobalAveragePooling1D()(y)\n",
    "    max_pool = GlobalMaxPooling1D()(y)\n",
    "    \n",
    "    conc = concatenate([atten_1, atten_2, avg_pool, max_pool])\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    outp = Dense(1, activation=\"sigmoid\")(conc)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "45e081dcb0c6d874945f3cb68c8e7c1b3e37457e"
   },
   "outputs": [],
   "source": [
    "def model_lstm_atten(embedding_matrix):\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    fea = Input(shape=(8,), name='Features')\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "    x = Attention(maxlen)(x)\n",
    "    x = concatenate([x,fea])\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=[inp,fea], outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "2ea730f2cf217567ff6df2d5869233d307b21403"
   },
   "outputs": [],
   "source": [
    "def cnn(embedding_matrix):\n",
    "    inp1 = Input(shape=(maxlen,))\n",
    "    fea = Input(shape=(6,), name='Features')\n",
    "    emb = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp1)\n",
    "    filters = []\n",
    "    for f in [3,4,5,10]:\n",
    "        conv = Conv1D(64,f, activation='elu')(emb)\n",
    "        pool = GlobalMaxPooling1D()(conv)\n",
    "        filters.append(pool)\n",
    "    \n",
    "    x = concatenate(filters)\n",
    "    x = concatenate([x,fea])\n",
    "    #classification dense net\n",
    "    x = Dense(maxlen, activation=\"relu\")(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[inp1,fea], outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=1e-3), metrics=[f1])\n",
    "#     print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "891ac032a4e41eb0c56e11f57e4108eb46f04fbc"
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "cf8d7f7c5eb0cd8661ef412eb9eff856a5fd1af2"
   },
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "3ef8c6fc9d37745a32a80b385bca950f732a305e"
   },
   "outputs": [],
   "source": [
    "def capsule(embedding_matrix):\n",
    "    K.clear_session()       \n",
    "    inp = Input(shape=(maxlen,))\n",
    "    fea = Input(shape=(8,), name='Features')\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(rate=0.2)(x)\n",
    "    x = Bidirectional(CuDNNGRU(100, return_sequences=True, \n",
    "                                kernel_initializer=glorot_normal(seed=12300), recurrent_initializer=orthogonal(gain=1.0, seed=10000)))(x)\n",
    "\n",
    "    x = Capsule(num_capsule=10, dim_capsule=10, routings=4, share_weights=True)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = concatenate([x,fea])\n",
    "    x = Dense(100, activation=\"relu\", kernel_initializer=glorot_normal(seed=12300))(x)\n",
    "    x = Dropout(0.12)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=[inp,fea], outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(),)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "557d19e8c1708d2fe015b38c3dd1964e46514074"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "a48142f06764bb772b6281a67dcd680711ecb294"
   },
   "outputs": [],
   "source": [
    "outputs=[]\n",
    "outputs.append([train_preds, test_preds_fold, th_f1['threshold'], 'Boss LSTM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "e19c193c50e4d124445a2524a39964b2f48f718d"
   },
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y,train_f,val_f = train_test_split(trainX, trainy, features, test_size=0.001,random_state=618)\n",
    "trn_idx = np.random.permutation(len(train_X))\n",
    "val_idx = np.random.permutation(len(val_X))\n",
    "train_X = train_X[trn_idx]\n",
    "val_X = val_X[val_idx]\n",
    "train_y = train_y[trn_idx]\n",
    "val_y = val_y[val_idx]\n",
    "train_f = train_f[trn_idx]\n",
    "val_f = val_f[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "3d30f020a688cea9271b3709d6722ce657804846"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      "1304815/1304815 [==============================] - 254s 195us/step - loss: 0.1084 - f1: 0.5980 - val_loss: 0.1194 - val_f1: 0.6765\n",
      "Epoch:  0 -    best Val F1 Score: 0.7143, best threshold: 0.3600\n",
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      "1304815/1304815 [==============================] - 253s 194us/step - loss: 0.0886 - f1: 0.6892 - val_loss: 0.1207 - val_f1: 0.6951\n",
      "Epoch:  1 -    best Val F1 Score: 0.7353, best threshold: 0.3600\n",
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      "1304815/1304815 [==============================] - 253s 194us/step - loss: 0.0738 - f1: 0.7493 - val_loss: 0.1226 - val_f1: 0.6817\n",
      "Epoch:  2 -    best Val F1 Score: 0.7193, best threshold: 0.3200\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97      1197\n",
      "           1       0.80      0.60      0.68       110\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1307\n",
      "   macro avg       0.88      0.79      0.83      1307\n",
      "weighted avg       0.95      0.95      0.95      1307\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "pred_val_y, pred_test_y, best_thresh,report= train_pred(model_gru_2pool(embedding_matrix), epochs = 3,fea=True)\n",
    "outputs.append([pred_val_y, pred_test_y, best_thresh, 'Gru 2pools'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "a05a15b66e16779393f3ed52d4a5f461b473dcfd"
   },
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y,train_f,val_f = train_test_split(trainX, trainy, features, test_size=0.001,random_state=618)\n",
    "trn_idx = np.random.permutation(len(train_X))\n",
    "val_idx = np.random.permutation(len(val_X))\n",
    "# #test\n",
    "# trn_idx = np.arange(100)\n",
    "# val_idx = np.arange(100)\n",
    "train_X = train_X[trn_idx]\n",
    "val_X = val_X[val_idx]\n",
    "train_y = train_y[trn_idx]\n",
    "val_y = val_y[val_idx]\n",
    "train_f = train_f[trn_idx]\n",
    "val_f = val_f[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "c07064f32eadbce19e629d48fde03e26f0fbc9c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      "1304815/1304815 [==============================] - 361s 276us/step - loss: 0.1131 - f1: 0.5784 - val_loss: 0.1214 - val_f1: 0.7230\n",
      "Epoch:  0 -    best Val F1 Score: 0.7347, best threshold: 0.4500\n",
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      "1304815/1304815 [==============================] - 359s 275us/step - loss: 0.0983 - f1: 0.6519 - val_loss: 0.1211 - val_f1: 0.6836\n",
      "Epoch:  1 -    best Val F1 Score: 0.7421, best threshold: 0.2700\n",
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      "1304815/1304815 [==============================] - 360s 276us/step - loss: 0.0915 - f1: 0.6794 - val_loss: 0.1120 - val_f1: 0.7200\n",
      "Epoch:  2 -    best Val F1 Score: 0.7463, best threshold: 0.4500\n",
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      "1304815/1304815 [==============================] - 360s 276us/step - loss: 0.0846 - f1: 0.7076 - val_loss: 0.1106 - val_f1: 0.7528\n",
      "Epoch:  3 -    best Val F1 Score: 0.7830, best threshold: 0.4100\n",
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      " 969728/1304815 [=====================>........] - ETA: 1:32 - loss: 0.0765 - f1: 0.7415"
     ]
    }
   ],
   "source": [
    "pred_val_y, pred_test_y, best_thresh,report = train_pred(model_lstm_atten(embedding_matrix), epochs = 5, fea=True)\n",
    "outputs.append([pred_val_y, pred_test_y, best_thresh, '2 LSTM w/ attention'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "4cec50bb19d3d92ecd866a15185b692ccb3cd623"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1197\n",
      "           1       0.81      0.67      0.74       110\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1307\n",
      "   macro avg       0.89      0.83      0.86      1307\n",
      "weighted avg       0.96      0.96      0.96      1307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "07b3f567fa7575fbce33202b5e05b2747565e4ad"
   },
   "outputs": [],
   "source": [
    "# train_X, val_X, train_y, val_y,train_f,val_f = train_test_split(trainX, trainy, features, test_size=0.001,random_state=618)\n",
    "# trn_idx = np.random.permutation(len(train_X))\n",
    "# val_idx = np.random.permutation(len(val_X))\n",
    "# train_X = train_X[trn_idx]\n",
    "# val_X = val_X[val_idx]\n",
    "# train_y = train_y[trn_idx]\n",
    "# val_y = val_y[val_idx]\n",
    "# train_f = train_f[trn_idx]\n",
    "# val_f = val_f[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "cd01188b4fb6ba2710be49fc4ced81e379356187"
   },
   "outputs": [],
   "source": [
    "# pred_val_y, pred_test_y, best_thresh, report = train_pred(model_lstm_atten2(embedding_matrix), epochs = 3)\n",
    "# outputs.append([pred_val_y, pred_test_y, best_thresh, '2 LSTM+GRU+atten'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "862a335c94176f0eae85dd52a91e9c15db65aecd"
   },
   "outputs": [],
   "source": [
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "4b2e5a021d36f19e8685a6642437d59e405525a5"
   },
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y,train_f,val_f = train_test_split(trainX, trainy, features, test_size=0.001,random_state=527)\n",
    "trn_idx = np.random.permutation(len(train_X))\n",
    "val_idx = np.random.permutation(len(val_X))\n",
    "train_X = train_X[trn_idx]\n",
    "val_X = val_X[val_idx]\n",
    "train_y = train_y[trn_idx]\n",
    "val_y = val_y[val_idx]\n",
    "train_f = train_f[trn_idx]\n",
    "val_f = val_f[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "dc2fb5ddea9e047ec85302613d76f80d21d328c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      "1304815/1304815 [==============================] - 236s 181us/step - loss: 0.1358 - val_loss: 0.1057\n",
      "Epoch:  0 -    best Val F1 Score: 0.7345, best threshold: 0.4800\n",
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      "1304815/1304815 [==============================] - 234s 179us/step - loss: 0.1050 - val_loss: 0.1051\n",
      "Epoch:  1 -    best Val F1 Score: 0.7551, best threshold: 0.2300\n",
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      "1304815/1304815 [==============================] - 234s 179us/step - loss: 0.0997 - val_loss: 0.0940\n",
      "Epoch:  2 -    best Val F1 Score: 0.7692, best threshold: 0.2200\n",
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      "1304815/1304815 [==============================] - 234s 179us/step - loss: 0.0958 - val_loss: 0.0869\n",
      "Epoch:  3 -    best Val F1 Score: 0.7941, best threshold: 0.3400\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1213\n",
      "           1       0.78      0.66      0.71        94\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1307\n",
      "   macro avg       0.87      0.82      0.85      1307\n",
      "weighted avg       0.96      0.96      0.96      1307\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "pred_val_y, pred_test_y, best_thresh, report = train_pred(capsule(embedding_matrix), epochs=4,fea=True)\n",
    "outputs.append([pred_val_y, pred_test_y, best_thresh, 'GRU Capsule concat ep4'])\n",
    "# pred_val_y, pred_test_y, best_thresh,report= train_pred(cnn(embedding_matrix), epochs = 10,fea=True)\n",
    "# outputs.append([pred_val_y, pred_test_y, best_thresh, 'Conv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "aabdc8427a4df528497df83fcb4c539ab304bf75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.24, 'Boss LSTM'), (0.32, 'Gru 2pools'), (0.41, '2 LSTM w/ attention'), (0.34, 'GRU Capsule concat ep4')]\n"
     ]
    }
   ],
   "source": [
    "print([(outputs[i][2],outputs[i][3]) for i in range(len(outputs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "262e34579dbcf793591c89c6e54567f435460b3c"
   },
   "outputs": [],
   "source": [
    "# coefs =[0.227,0.1900,0.2024,0.1907,0.20]\n",
    "coefs =[0.277,0.2400,0.2424,0.25]\n",
    "ths = np.sum([outputs[i][2]*coefs[i] for i in range(len(coefs))], axis = 0)\n",
    "pred_test_y = np.sum([outputs[i][1].reshape(-1,1)*coefs[i] for i in range(len(coefs))], axis = 0)\n",
    "\n",
    "pred_test_y = (pred_test_y > ths).astype(int)\n",
    "test_df = pd.read_csv(\"../input/test.csv\", usecols=[\"qid\"])\n",
    "out_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n",
    "out_df['prediction'] = pred_test_y\n",
    "out_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "c6fbd7c30caeacd2d8f6d7936c014202b396d593"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.327664"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.sum([outputs[i][2]*coefs[i] for i in range(len(coefs))], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "The part is the local analysis for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T08:06:42.931594Z",
     "start_time": "2019-06-03T08:06:42.927662Z"
    },
    "_kg_hide-input": true,
    "_uuid": "a8a37bd3c45e1907372b654de0316ff2b22b7e66"
   },
   "outputs": [],
   "source": [
    "# Analysis\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "toxic = train_df[train_df['target']==1][['qid','question_text']]\n",
    "nontoxic = train_df[train_df['target']==0][['qid','question_text']]\n",
    "\n",
    "def printwordc(li):\n",
    "    w_score = []\n",
    "    for l in li:\n",
    "        score = toxd[l]/(nontoxd[l]*10)\n",
    "        w_score.append((l,score))\n",
    "    return w_score\n",
    "\n",
    "def post_preprocess(df):\n",
    "    i = 0\n",
    "    post_list = []\n",
    "    length = len(df)\n",
    "    stemmer = PorterStemmer()\n",
    "    print('Processing... Be patient')\n",
    "\n",
    "    for row in df.iterrows():\n",
    "        # Progress bar\n",
    "        i += 1\n",
    "        if (i % 500 == 0 or i == length):\n",
    "            print(f\"Progress bar：{round(i/length*100)}%\")\n",
    "        # clean the posts\n",
    "        posts = row[1].question_text\n",
    "        posts = re.sub(r'\\|\\|\\|', ' ', posts)\n",
    "        posts = re.sub(r'http[\\S]*', '', posts).lower()\n",
    "        posts = re.sub(\"[^a-z\\s]\", ' ', posts)\n",
    "        posts = ' '.join([stemmer.stem(w) for w in posts.split(\n",
    "            ' ') if w not in stopwords.words('english')])\n",
    "\n",
    "        post_list.append(posts)\n",
    "\n",
    "    return np.array(post_list)\n",
    "\n",
    "## Toxic and Non-toxic Analysis\n",
    "process_toxic = post_preprocess(toxic)\n",
    "process_nontoxic = post_preprocess(nontoxic)\n",
    "nontoxic_sam = nontoxic.sample(frac=1/10,replace=False)\n",
    "import collections\n",
    "frequency = collections.defaultdict(int)\n",
    "for i in tqdm(range(len(process_toxic))):\n",
    "    text = process_toxic[i]\n",
    "    for word in text.split():\n",
    "        frequency[word] += 1\n",
    "     \n",
    "nt_frequency = collections.defaultdict(int)\n",
    "for i in tqdm(range(len(process_nontoxic))):\n",
    "    text = process_nontoxic[i]\n",
    "    for word in text.split():\n",
    "        nt_frequency[word] += 1\n",
    "       \n",
    "freq_sort = sorted(frequency.items(),key=lambda x:-x[1])\n",
    "nt_freq_sort = sorted(nt_frequency.items(),key=lambda x:-x[1])\n",
    "nontoxd = dict(nt_freq_sort)\n",
    "toxd = dict(freq_sort)\n",
    "\n",
    "top50 = []\n",
    "for n in freq_sort[:50]:\n",
    "    top50.append(n[0])\n",
    "w_score = sorted(printwordc(top50),key=lambda x:x[1],reverse=True)\n",
    "\n",
    "real_toxic_word = []\n",
    "for key in toxd:\n",
    "    if key not in nontoxd:\n",
    "        real_toxic_word.append(key)\n",
    "        \n",
    "real_toxic_dict=[]\n",
    "for r in real_toxic_word:\n",
    "    real_toxic_dict.append((r,toxd[r]))\n",
    "real_toxic_dict = sorted(real_toxic_dict,key=lambda x:x[1],reverse=True)\n",
    "\n",
    "real_toxic_top500 = []\n",
    "for tp in real_toxic_dict[:500]:\n",
    "    real_toxic_top500.append(tp[0])  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
